# .github/workflows/llm-pr-review.yml
name: LLM PR Review

on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  review:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Fetch diff
        id: get_diff
        run: |
          DIFF_URL="${{ github.event.pull_request.diff_url }}"
          DIFF=$(curl -s -H "Accept: application/vnd.github.v3.diff" "$DIFF_URL")
          echo "::set-output name=diff::$DIFF"

      - name: Load configuration
        id: load_config
        uses: actions/github-script@v6
        env:
          CONFIG_FILE: .github/config.yml # or .github/config.json - YAML preferred for readability
        with:
          script: |
            const core = require('@actions/core');
            const yaml = require('js-yaml');
            const fs = require('fs');
            const configPath = process.env.CONFIG_FILE;
            let config = {};

            core.info(`Attempting to load configuration from: ${configPath}`);

            try {
              if (fs.existsSync(configPath)) {
                const fileContent = fs.readFileSync(configPath, 'utf8');
                if (configPath.endsWith('.yml') || configPath.endsWith('.yaml')) {
                  config = yaml.load(fileContent);
                } else if (configPath.endsWith('.json')) {
                  config = JSON.parse(fileContent);
                } else {
                  core.warning(`Configuration file '${configPath}' extension not recognized. Assuming YAML.`);
                  config = yaml.load(fileContent); // Try YAML parsing as default
                }
                core.info(`Configuration loaded successfully from: ${configPath}`);
                core.setOutput('analysis_prompt', config.analysis_prompt);
                core.setOutput('action_prompt', config.action_prompt);
                core.setOutput('llm_model', config.llm_model || 'gpt-3.5-turbo'); # Default model if not in config
                core.setOutput('llm_api_url', config.llm_api_url || 'https://api.openai.com/v1/chat/completions'); # Default OpenAI URL
                core.setOutput('meta_analysis_system_prompt', config.meta_analysis_system_prompt); # Load meta prompts
                core.setOutput('meta_action_system_prompt', config.meta_action_system_prompt);
              } else {
                core.info(`Configuration file not found at: ${configPath}. Falling back to secrets.`);
                core.setFailed('config-file-not-found'); # Indicate config file load failure for conditional step
              }
            } catch (error) {
              core.warning(`Error loading configuration file '${configPath}': ${error.message}. Falling back to secrets.`);
              core.setFailed('config-load-error'); # Indicate config load failure
            }

      - name: Set configuration from secrets (fallback)
        if: steps.load_config.outcome == 'failure'
        id: set_config_from_secrets
        run: |
          echo "::set-output name=analysis_prompt::${{ secrets.ANALYSIS_PROMPT }}"
          echo "::set-output name=action_prompt::${{ secrets.ACTION_PROMPT }}"
          echo "::set-output name=llm_model::${{ secrets.LLM_MODEL || 'gpt-3.5-turbo' }}" # Secret for model, default as fallback
          echo "::set-output name=llm_api_url::${{ secrets.LLM_API_URL || 'https://api.openai.com/v1/chat/completions' }}" # Secret for API URL, default as fallback
          echo "::set-output name=meta_analysis_system_prompt::${{ secrets.META_ANALYSIS_SYSTEM_PROMPT }}" # Secrets for meta prompts
          echo "::set-output name=meta_action_system_prompt::${{ secrets.META_ACTION_SYSTEM_PROMPT }}"

          if [[ -z "${{ secrets.ANALYSIS_PROMPT }}" || -z "${{ secrets.ACTION_PROMPT }}" ]]; then
            echo "::error::Prompts not found in secrets or config file. Please configure ANALYSIS_PROMPT and ACTION_PROMPT."
            exit 1 # Exit workflow if prompts are missing
          fi

      - name: LLM Analysis
        id: llm_analysis
        uses: actions/github-script@v6
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANALYSIS_PROMPT: ${{ steps.load_config.outputs.analysis_prompt || steps.set_config_from_secrets.outputs.analysis_prompt }}
          LLM_MODEL: ${{ steps.load_config.outputs.llm_model || steps.set_config_from_secrets.outputs.llm_model }}
          LLM_API_URL: ${{ steps.load_config.outputs.llm_api_url || steps.set_config_from_secrets.outputs.llm_api_url }}
          DIFF: ${{ steps.get_diff.outputs.diff }}
          META_ANALYSIS_SYSTEM_PROMPT: ${{ steps.load_config.outputs.meta_analysis_system_prompt || secrets.META_ANALYSIS_SYSTEM_PROMPT }} # Fallback to secret for meta prompt
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const core = require('@actions/core');
            const fetch = require('node-fetch');
            const analysisPrompt = process.env.ANALYSIS_PROMPT;
            const diff = process.env.DIFF;
            const apiKey = process.env.OPENAI_API_KEY;
            const llmModel = process.env.LLM_MODEL;
            const llmApiUrl = process.env.LLM_API_URL;
            const metaAnalysisSystemPrompt = process.env.META_ANALYSIS_SYSTEM_PROMPT; // Get meta system prompt

            if (!apiKey) {
              core.setFailed("OPENAI_API_KEY secret is required for LLM API access.");
              return;
            }

            core.info(`Using LLM API URL: ${llmApiUrl}`);
            core.info(`Using LLM Model: ${llmModel}`);

            const payload = {
              model: llmModel,
              messages: [
                {"role": "system", "content": metaAnalysisSystemPrompt || `You are an expert code reviewer. Analyze the following diff.`}, // Use meta system prompt, default if not provided
                {"role": "user", "content": `${analysisPrompt}\n\nDiff:\n${diff}`}
              ]
            };

            try {
              const response = await fetch(llmApiUrl, {
                method: 'POST',
                headers: {
                  'Content-Type': 'application/json',
                  'Authorization': `Bearer ${apiKey}`
                },
                body: JSON.stringify(payload)
              });

              if (!response.ok) {
                  const errorDetails = await response.json();
                  core.error(`LLM API Error: ${response.status} - ${response.statusText}`);
                  core.error(`Error details: ${JSON.stringify(errorDetails)}`);
                  core.setFailed(`LLM analysis failed with status code: ${response.status}. See error details in logs.`);
                  return;
              }

              const data = await response.json();
              const analysis = data.choices[0].message.content.trim();
              core.setOutput('analysis', analysis);
              core.info('LLM Analysis step completed successfully.');

            } catch (error) {
              core.error("Error calling LLM API:");
              core.error(error);
              core.setFailed(`LLM analysis step failed due to an error: ${error.message}`);
            }


      - name: LLM Action
        id: llm_action
        uses: actions/github-script@v6
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ACTION_PROMPT: ${{ steps.load_config.outputs.action_prompt || steps.set_config_from_secrets.outputs.action_prompt }}
          LLM_MODEL: ${{ steps.load_config.outputs.llm_model || steps.set_config_from_secrets.outputs.llm_model }}
          LLM_API_URL: ${{ steps.load_config.outputs.llm_api_url || steps.set_config_from_secrets.outputs.llm_api_url }}
          ANALYSIS: ${{ steps.llm_analysis.outputs.analysis }}
          DIFF: ${{ steps.get_diff.outputs.diff }}
          META_ACTION_SYSTEM_PROMPT: ${{ steps.load_config.outputs.meta_action_system_prompt || secrets.META_ACTION_SYSTEM_PROMPT }} # Fallback to secret for meta prompt
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const core = require('@actions/core');
            const fetch = require('node-fetch');
            const actionPrompt = process.env.ACTION_PROMPT;
            const analysis = process.env.ANALYSIS;
            const diff = process.env.DIFF;
            const apiKey = process.env.OPENAI_API_KEY;
            const llmModel = process.env.LLM_MODEL;
            const llmApiUrl = process.env.LLM_API_URL;
            const metaActionSystemPrompt = process.env.META_ACTION_SYSTEM_PROMPT; // Get meta system prompt


            if (!apiKey) {
              core.setFailed("OPENAI_API_KEY secret is required for LLM API access.");
              return;
            }
            core.info(`Using LLM API URL: ${llmApiUrl}`);
            core.info(`Using LLM Model: ${llmModel}`);


            const payload = {
              model: llmModel,
              messages: [
                {"role": "system", "content": metaActionSystemPrompt || `You are a helpful code review assistant. Generate a PR comment based on the analysis.`}, // Use meta system prompt, default if not provided
                {"role": "user", "content": `${actionPrompt}\n\nAnalysis Summary:\n${analysis}\n\nFull Diff:\n${diff}`}
              ]
            };

            try {
              const response = await fetch(llmApiUrl, {
                method: 'POST',
                headers: {
                  'Content-Type': 'application/json',
                  'Authorization': `Bearer ${apiKey}`
                },
                body: JSON.stringify(payload)
              });

              if (!response.ok) {
                  const errorDetails = await response.json();
                  core.error(`LLM API Error: ${response.status} - ${response.statusText}`);
                  core.error(`Error details: ${JSON.stringify(errorDetails)}`);
                  core.setFailed(`LLM action failed with status code: ${response.status}. See error details in logs.`);
                  return;
              }

              const data = await response.json();
              const message = data.choices[0].message.content.trim();
              core.setOutput('message', message);
              core.info('LLM Action step completed successfully.');

            } catch (error) {
              core.error("Error calling LLM API:");
              core.error(error);
              core.setFailed(`LLM action step failed due to an error: ${error.message}`);
            }

      - name: Post comment to PR
        uses: actions/github-script@v6
        env:
          MESSAGE: ${{ steps.llm_action.outputs.message }}
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const core = require('@actions/core');
            const message = process.env.MESSAGE;
            if (!message) {
              core.warning('No comment message generated by LLM Action step. Skipping comment posting.');
              return; // Skip posting if no message
            }

            try {
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `**LLM PR Review:**\n\n${message}` # Add a title to the comment
              });
              core.info("Comment posted to PR successfully.");
            } catch (error) {
              core.error("Error posting comment to PR:");
              core.error(error);
              core.setFailed(`Failed to post comment to PR: ${error.message}`);
            }
